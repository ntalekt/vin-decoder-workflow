#!/usr/bin/env python3
"""
Bring a Trailer scraper for Porsche 911 listings (1981+).
- Scrapes from BOTH the main /porsche/911/ page AND the auction results search
- Gets comprehensive historical data by using BaT's search functionality
- Only accepts cars with valid WP0* VINs (no parts/memorabilia)
- Enforces YYYY-porsche-911-* URL pattern for 1981+ models
"""

import argparse
import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, urlparse, quote

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from utils import setup_logging, save_json_file, create_timestamp, validate_vin

class BaTScraper:
    """Bring a Trailer scraper for Porsche 911 listings."""
    
    def __init__(self, max_runtime_minutes: int = 45):
        self.logger = setup_logging()
        self.base_url = "https://bringatrailer.com"
        self.max_runtime = timedelta(minutes=max_runtime_minutes)
        self.start_time = datetime.now()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'
        })
        self.processed_vins = set()
    
    def configure_chrome_driver(self) -> webdriver.Chrome:
        """Configure Chrome driver for BaT scraping."""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    
    def should_continue(self) -> bool:
        """Check if scraping should continue based on time limit."""
        elapsed = datetime.now() - self.start_time
        return elapsed < self.max_runtime
    
    def search_auction_results(self) -> List[str]:
        """
        Search the auction results page for Porsche 911 listings with pagination.
        This gets access to the full historical archive.
        """
        self.logger.info("Searching auction results for Porsche 911 historical data")
        listing_urls = set()
        driver = None
        
        try:
            driver = self.configure_chrome_driver()
            
            # Go to auction results with Porsche search
            search_url = f"{self.base_url}/auctions/results/?search=porsche+911"
            self.logger.info(f"Loading auction results search: {search_url}")
            driver.get(search_url)
            
            # Wait for page to load
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(5)
            
            def collect_results_page():
                """Collect all listing URLs from current results page."""
                current_links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/listing/"]')
                new_urls_added = 0
                
                for link in current_links:
                    try:
                        href = link.get_attribute('href')
                        if href and self.is_valid_listing_url(href):
                            if href not in listing_urls:
                                listing_urls.add(href)
                                new_urls_added += 1
                                self.logger.info(f"DEBUG: Added auction result: {href}")
                    except Exception as e:
                        self.logger.debug(f"Error processing results link: {e}")
                        continue
                
                return new_urls_added
            
            # Collect initial results
            initial_count = collect_results_page()
            self.logger.info(f"Initial auction results collection: {initial_count} valid listings")
            
            # Look for pagination and load more results
            page_clicks = 0
            max_pages = 50  # Limit to prevent infinite loops
            
            while page_clicks < max_pages and self.should_continue():
                try:
                    # Look for "Load More" or "Next Page" buttons
                    load_more_buttons = driver.find_elements(
                        By.XPATH,
                        "//*[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'load more') or "
                        "contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'next') or "
                        "contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'see more')]"
                    )
                    
                    clickable_button = None
                    for button in load_more_buttons:
                        try:
                            if button.is_displayed() and button.is_enabled():
                                clickable_button = button
                                break
                        except:
                            continue
                    
                    if not clickable_button:
                        self.logger.info("No more pagination buttons found in results")
                        break
                    
                    pre_click_count = len(listing_urls)
                    self.logger.info(f"Clicking pagination button #{page_clicks + 1}")
                    
                    # Scroll and click
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", clickable_button)
                    time.sleep(1)
                    
                    try:
                        clickable_button.click()
                    except:
                        driver.execute_script("arguments[0].click();", clickable_button)
                    
                    time.sleep(4)  # Wait for new results to load
                    
                    # Collect new results
                    new_results = collect_results_page()
                    post_click_count = len(listing_urls)
                    actual_new = post_click_count - pre_click_count
                    
                    self.logger.info(f"Found {actual_new} new auction results (total: {post_click_count})")
                    
                    if actual_new == 0:
                        break
                    
                    page_clicks += 1
                    
                except Exception as e:
                    self.logger.warning(f"Error clicking pagination: {e}")
                    break
            
            self.logger.info(f"Auction results search completed after {page_clicks} pages")
            
        except Exception as e:
            self.logger.error(f"Error searching auction results: {e}")
        
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
        
        return list(listing_urls)
    
    def search_porsche_911_listings(self) -> List[str]:
        """
        Search for Porsche 911 listings from both the main page and auction results.
        This provides comprehensive coverage of active and historical listings.
        """
        self.logger.info("Searching for Porsche 911 listings from multiple sources")
        all_listing_urls = set()
        
        # Method 1: Search the main Porsche 911 page for current/featured listings
        self.logger.info("Phase 1: Searching main Porsche 911 page")
        main_page_urls = self._search_main_page()
        all_listing_urls.update(main_page_urls)
        self.logger.info(f"Main page found: {len(main_page_urls)} listings")
        
        # Method 2: Search auction results for historical data
        self.logger.info("Phase 2: Searching auction results for historical data")
        results_urls = self.search_auction_results()
        all_listing_urls.update(results_urls)
        self.logger.info(f"Auction results found: {len(results_urls)} listings")
        
        unique_urls = list(all_listing_urls)
        self.logger.info(f"Total unique listings found: {len(unique_urls)}")
        
        return unique_urls
    
    def _search_main_page(self) -> List[str]:
        """Search the main Porsche 911 page for current listings."""
        listing_urls = set()
        driver = None
        
        try:
            driver = self.configure_chrome_driver()
            
            # Go to the Porsche 911 page
            porsche_url = f"{self.base_url}/porsche/911/"
            driver.get(porsche_url)
            
            # Wait for page to load
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(5)
            
            def collect_main_listings():
                """Collect listings from main page."""
                current_links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/listing/"]')
                new_urls_added = 0
                
                for link in current_links:
                    try:
                        href = link.get_attribute('href')
                        if href and self.is_valid_listing_url(href):
                            if href not in listing_urls:
                                listing_urls.add(href)
                                new_urls_added += 1
                    except:
                        continue
                
                return new_urls_added
            
            # Initial collection
            collect_main_listings()
            
            # Try clicking Show More buttons
            show_more_clicks = 0
            max_clicks = 10
            
            while show_more_clicks < max_clicks and self.should_continue():
                try:
                    show_more_buttons = driver.find_elements(
                        By.XPATH,
                        "//*[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'show more')]"
                    )
                    
                    clickable_button = next(
                        (b for b in show_more_buttons if b.is_displayed() and b.is_enabled()), 
                        None
                    )
                    
                    if not clickable_button:
                        break
                    
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", clickable_button)
                    time.sleep(1)
                    
                    try:
                        clickable_button.click()
                    except:
                        driver.execute_script("arguments[0].click();", clickable_button)
                    
                    time.sleep(4)
                    
                    new_listings = collect_main_listings()
                    if new_listings == 0:
                        break
                    
                    show_more_clicks += 1
                    
                except Exception:
                    break
            
        except Exception as e:
            self.logger.error(f"Error searching main page: {e}")
        
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
        
        return list(listing_urls)
    
    def is_valid_listing_url(self, url: str) -> bool:
        """
        Check if URL is a valid BaT listing for a 1981+ Porsche 911.
        Must match exact pattern: YYYY-porsche-911-*
        """
        if not url or '/listing/' not in url:
            return False
        
        try:
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            
            if len(path_parts) >= 2 and path_parts[0] == 'listing':
                listing_name = path_parts[1]
                
                # STRICT PATTERN: Must be YYYY-porsche-911-*
                pattern = r'^(19[8-9]\d|20[0-2]\d)-porsche-911-'
                if not re.match(pattern, listing_name, re.IGNORECASE):
                    return False
                
                # Extract and validate year
                year_match = re.match(r'^(\d{4})', listing_name)
                if year_match:
                    year = int(year_match.group(1))
                    if year < 1981:
                        return False
                
                return True
        
        except Exception:
            return False
        
        return False
    
    def extract_vin_from_text(self, text: str) -> Optional[str]:
        """Extract 17-digit VIN starting with WP0 from text content."""
        # Pattern for 17-digit VIN
        vin_pattern = r'\b[A-HJ-NPR-Z0-9]{17}\b'
        matches = re.findall(vin_pattern, text, re.IGNORECASE)
        
        for match in matches:
            vin = match.upper()
            # Only accept Porsche VINs starting with WP0
            if vin.startswith('WP0') and validate_vin(vin):
                return vin
        
        return None
    
    def extract_price_from_text(self, text: str) -> Dict[str, Any]:
        """Extract price information from text."""
        price_info = {
            'current_bid': '',
            'reserve_met': False,
            'buy_it_now': '',
            'sold_price': '',
            'no_reserve': False
        }
        
        text_lower = text.lower()
        
        # Current bid patterns
        bid_patterns = [
            r'bid:\s*usd\s*\$[\d,]+',
            r'current bid[:\s]*\$[\d,]+',
            r'\$[\d,]+\s*current bid',
            r'bid[:\s]*\$[\d,]+',
        ]
        
        for pattern in bid_patterns:
            match = re.search(pattern, text_lower)
            if match:
                price_match = re.search(r'\$[\d,]+', match.group())
                if price_match:
                    price_info['current_bid'] = price_match.group()
                    break
        
        # Reserve status
        if 'reserve met' in text_lower or 'reserve has been met' in text_lower:
            price_info['reserve_met'] = True
        
        if 'no reserve' in text_lower or 'no-reserve' in text_lower:
            price_info['no_reserve'] = True
        
        # Sold price (for auction results)
        sold_patterns = [
            r'sold for \$[\d,]+',
            r'winning bid[:\s]*\$[\d,]+',
            r'final bid[:\s]*\$[\d,]+',
            r'hammer price[:\s]*\$[\d,]+',
        ]
        
        for pattern in sold_patterns:
            match = re.search(pattern, text_lower)
            if match:
                price_match = re.search(r'\$[\d,]+', match.group())
                if price_match:
                    price_info['sold_price'] = price_match.group()
                    break
        
        return price_info
    
    def determine_auction_status(self, page_text: str) -> str:
        """Determine if this is an active auction, sold auction, or ended without sale."""
        text_lower = page_text.lower()
        
        # Check for sold indicators first (most definitive)
        sold_indicators = [
            'sold for $',
            'winning bid:',
            'final bid:',
            'hammer price:',
            'sale completed',
            'auction ended',
            'congratulations to'
        ]
        
        for indicator in sold_indicators:
            if indicator in text_lower:
                return 'sold'
        
        # Check for active auction indicators
        active_indicators = [
            'current bid:',
            'bid: usd $',
            'time left:',
            'days left',
            'hours left',
            'minutes left',
            'bidding ends',
            'reserve not met',
            'reserve met'
        ]
        
        for indicator in active_indicators:
            if indicator in text_lower:
                return 'active'
        
        # Check for ended without sale
        ended_indicators = [
            'reserve not met',
            'auction ended without sale',
            'no sale',
            'did not meet reserve'
        ]
        
        for indicator in ended_indicators:
            if indicator in text_lower:
                return 'ended'
        
        # Default to unknown if we can't determine
        return 'unknown'
    
    def scrape_listing_details(self, listing_url: str) -> Optional[Dict[str, Any]]:
        """Scrape detailed information from a BaT listing."""
        if not self.should_continue():
            return None
        
        self.logger.info(f"Scraping listing: {listing_url}")
        
        driver = None
        try:
            driver = self.configure_chrome_driver()
            driver.get(listing_url)
            
            # Wait for page to load
            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            # Give page time to fully load
            time.sleep(3)
            
            # Get page source and parse
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # Get page text for analysis
            page_text = soup.get_text(separator=' ', strip=True)
            page_text_lower = page_text.lower()
            
            # Verify this is actually a Porsche 911 listing
            if not ('porsche' in page_text_lower and '911' in page_text_lower):
                self.logger.info(f"Not a Porsche 911 listing, skipping: {listing_url}")
                return None
            
            # CRITICAL: Extract VIN and verify it's a WP0 VIN
            vin = self.extract_vin_from_text(page_text)
            if not vin:
                self.logger.info(f"No valid WP0 VIN found, skipping: {listing_url}")
                return None
            
            if vin in self.processed_vins:
                self.logger.info(f"VIN {vin} already processed, skipping")
                return None
            
            self.processed_vins.add(vin)
            
            # Extract basic listing info
            listing_data = {
                'url': listing_url,
                'scraped_at': create_timestamp(),
                'source': 'BringATrailer',
                'listing_id': '',
                'title': '',
                'year': '',
                'make': 'PORSCHE',
                'model': '911',
                'vin': vin,
                'mileage': '',
                'location': '',
                'seller_type': '',
                'auction_status': 'unknown',
                'end_date': '',
                'price_info': {},
                'description': '',
                'specifications': {},
                'features': [],
                'condition_notes': [],
                'service_history': [],
                'modifications': [],
                'photos': []
            }
            
            # Extract listing ID from URL
            url_parts = urlparse(listing_url).path.split('/')
            if len(url_parts) > 2:
                listing_data['listing_id'] = url_parts[2]
            
            # Determine auction status using improved logic
            listing_data['auction_status'] = self.determine_auction_status(page_text)
            
            # Title and basic info
            title_selectors = ['h1', '.listing-title', '.auction-title', 'title']
            for selector in title_selectors:
                title_elem = soup.find(selector)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text and len(title_text) > 5:  # Valid title
                        listing_data['title'] = title_text
                        # Extract year from title
                        year_match = re.search(r'\b(19|20)\d{2}\b', title_text)
                        if year_match:
                            listing_data['year'] = year_match.group()
                        break
            
            # Price information
            listing_data['price_info'] = self.extract_price_from_text(page_text)
            
            # Basic extraction for other fields...
            # (keeping this concise for the main improvement)
            
            self.logger.info(f"Successfully scraped listing for VIN {vin} (Status: {listing_data['auction_status']})")
            return listing_data
            
        except Exception as e:
            self.logger.error(f"Error scraping listing {listing_url}: {e}")
            return None
        
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def normalize_bat_record(self, listing_data: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize BaT listing data to match inventory schema."""
        # Extract year as integer
        year = 0
        if listing_data.get('year') and listing_data['year'].isdigit():
            year = int(listing_data['year'])
        
        # Calculate age
        current_year = datetime.now().year
        age = current_year - year if year > 0 else 0
        
        normalized = {
            # Primary identifiers
            'vin': listing_data.get('vin', ''),
            'make': 'PORSCHE',
            'model': '911',
            'model_year': str(year) if year > 0 else '',
            'manufacturer': 'DR. ING. H.C.F. PORSCHE AG',
            
            # BaT specific data
            'bat_listing_id': listing_data.get('listing_id', ''),
            'bat_url': listing_data.get('url', ''),
            'bat_title': listing_data.get('title', ''),
            'bat_auction_status': listing_data.get('auction_status', ''),
            'bat_description': listing_data.get('description', ''),
            
            # Price information
            'bat_current_bid': listing_data.get('price_info', {}).get('current_bid', ''),
            'bat_sold_price': listing_data.get('price_info', {}).get('sold_price', ''),
            
            # Metadata
            'source': 'BringATrailer',
            'first_scraped': listing_data.get('scraped_at', ''),
            'last_updated': create_timestamp(),
            'scrape_count': 1,
            
            # Age classification
            'age_years': age,
            'is_classic': age > 15,
            'is_vintage': age > 25,
        }
        
        return normalized


def scrape_bat_listings(max_runtime_minutes: int = 45) -> Dict[str, Any]:
    """Scrape BaT for Porsche 911 listings."""
    logger = setup_logging()
    scraper = BaTScraper(max_runtime_minutes)
    
    # Results storage
    results = {
        'metadata': {
            'scrape_started': create_timestamp(),
            'scrape_completed': '',
            'runtime_minutes': 0,
            'total_listings_found': 0,
            'total_listings_scraped': 0,
            'listings_with_vins': 0,
            'new_vins': 0,
            'updated_vins': 0,
            'source': 'BringATrailer',
            'target': 'Porsche 911 (1981+) - Comprehensive Search'
        },
        'listings': []
    }
    
    try:
        # Search for listings using comprehensive approach
        listing_urls = scraper.search_porsche_911_listings()
        results['metadata']['total_listings_found'] = len(listing_urls)
        
        # Scrape individual listings
        scraped_count = 0
        vins_found = 0
        
        for url in listing_urls:
            if not scraper.should_continue():
                logger.info("Time limit reached, stopping scrape")
                break
            
            listing_data = scraper.scrape_listing_details(url)
            if listing_data:
                scraped_count += 1
                if listing_data.get('vin'):
                    vins_found += 1
                
                normalized_record = scraper.normalize_bat_record(listing_data)
                results['listings'].append(normalized_record)
                
                # Small delay between listings
                time.sleep(2)  # Faster since we're getting more listings
        
        results['metadata']['total_listings_scraped'] = scraped_count
        results['metadata']['listings_with_vins'] = vins_found
        results['metadata']['new_vins'] = vins_found
        
    except Exception as e:
        logger.error(f"Scraping failed: {e}")
        results['metadata']['error'] = str(e)
    
    finally:
        # Calculate runtime
        end_time = datetime.now()
        runtime = (end_time - scraper.start_time).total_seconds() / 60
        results['metadata']['runtime_minutes'] = round(runtime, 2)
        results['metadata']['scrape_completed'] = create_timestamp()
    
    return results


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description='Scrape Bring a Trailer for Porsche 911 listings')
    parser.add_argument('--max-runtime', type=int, default=45,
                        help='Maximum runtime in minutes (default: 45)')
    parser.add_argument('--output', default='bat-porsche-911-listings.json',
                        help='Output filename')
    args = parser.parse_args()
    
    logger = setup_logging()
    
    try:
        logger.info(f"Starting comprehensive BaT scrape with {args.max_runtime} minute limit")
        
        # Scrape listings
        results = scrape_bat_listings(args.max_runtime)
        
        # Save results
        save_json_file(results, args.output)
        
        # Print summary
        metadata = results['metadata']
        print(f"‚úÖ BaT comprehensive scraping completed")
        print(f"üìÅ Results saved to {args.output}")
        print(f"‚è±Ô∏è Runtime: {metadata['runtime_minutes']} minutes")
        print(f"üîç Listings found: {metadata['total_listings_found']}")
        print(f"üìä Listings scraped: {metadata['total_listings_scraped']}")
        print(f"üöó Valid WP0 VINs collected: {metadata['listings_with_vins']}")
        
        # Show status breakdown
        if results['listings']:
            active_count = sum(1 for listing in results['listings'] 
                             if listing.get('bat_auction_status') == 'active')
            sold_count = sum(1 for listing in results['listings'] 
                           if listing.get('bat_auction_status') == 'sold')
            ended_count = sum(1 for listing in results['listings'] 
                            if listing.get('bat_auction_status') == 'ended')
            unknown_count = sum(1 for listing in results['listings'] 
                              if listing.get('bat_auction_status') == 'unknown')
            
            print(f"üìà Active auctions: {active_count}")
            print(f"‚úÖ Sold auctions: {sold_count}")
            print(f"‚èπÔ∏è Ended auctions: {ended_count}")
            print(f"‚ùì Unknown status: {unknown_count}")
            
            years = [int(listing.get('model_year', '0')) for listing in results['listings'] 
                    if listing.get('model_year', '').isdigit()]
            if years:
                print(f"üìÖ Year range: {min(years)}-{max(years)}")
        
        return 0
    
    except Exception as e:
        logger.error(f"BaT scraping failed: {e}")
        print(f"‚ùå Error: {e}", file=sys.stderr)
        return 1


if __name__ == '__main__':
    sys.exit(main())
